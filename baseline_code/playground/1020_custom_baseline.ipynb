{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **PIP install**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: segmentation_models_pytorch in /opt/conda/envs/segmentation/lib/python3.7/site-packages (0.2.0)\n",
      "Requirement already satisfied: efficientnet-pytorch==0.6.3 in /opt/conda/envs/segmentation/lib/python3.7/site-packages (from segmentation_models_pytorch) (0.6.3)\n",
      "Requirement already satisfied: torchvision>=0.5.0 in /opt/conda/envs/segmentation/lib/python3.7/site-packages (from segmentation_models_pytorch) (0.10.1)\n",
      "Requirement already satisfied: pretrainedmodels==0.7.4 in /opt/conda/envs/segmentation/lib/python3.7/site-packages (from segmentation_models_pytorch) (0.7.4)\n",
      "Requirement already satisfied: timm==0.4.12 in /opt/conda/envs/segmentation/lib/python3.7/site-packages (from segmentation_models_pytorch) (0.4.12)\n",
      "Requirement already satisfied: torch in /opt/conda/envs/segmentation/lib/python3.7/site-packages (from efficientnet-pytorch==0.6.3->segmentation_models_pytorch) (1.9.1)\n",
      "Requirement already satisfied: munch in /opt/conda/envs/segmentation/lib/python3.7/site-packages (from pretrainedmodels==0.7.4->segmentation_models_pytorch) (2.5.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/envs/segmentation/lib/python3.7/site-packages (from pretrainedmodels==0.7.4->segmentation_models_pytorch) (4.28.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/envs/segmentation/lib/python3.7/site-packages (from torch->efficientnet-pytorch==0.6.3->segmentation_models_pytorch) (3.10.0.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/envs/segmentation/lib/python3.7/site-packages (from torchvision>=0.5.0->segmentation_models_pytorch) (1.21.2)\n",
      "Requirement already satisfied: pillow>=5.3.0 in /opt/conda/envs/segmentation/lib/python3.7/site-packages (from torchvision>=0.5.0->segmentation_models_pytorch) (8.3.2)\n",
      "Requirement already satisfied: six in /opt/conda/envs/segmentation/lib/python3.7/site-packages (from munch->pretrainedmodels==0.7.4->segmentation_models_pytorch) (1.16.0)\n",
      "Requirement already satisfied: sklearn in /opt/conda/envs/segmentation/lib/python3.7/site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/envs/segmentation/lib/python3.7/site-packages (from sklearn) (1.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /opt/conda/envs/segmentation/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.7.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/envs/segmentation/lib/python3.7/site-packages (from scikit-learn->sklearn) (3.0.0)\n",
      "Requirement already satisfied: numpy>=1.14.6 in /opt/conda/envs/segmentation/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.21.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/envs/segmentation/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install segmentation_models_pytorch\n",
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Import modules and Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T05:54:58.701660Z",
     "start_time": "2021-10-04T05:54:58.680659Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch version: 1.9.1+cu102\n",
      "GPU: True\n",
      "Device name:  Tesla V100-SXM2-32GB\n",
      "Device count:  1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 전처리를 위한 라이브러리\n",
    "from pycocotools.coco import COCO\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations import RandomResizedCrop\n",
    "from albumentations.augmentations.crops.transforms import RandomCrop\n",
    "from albumentations.augmentations.geometric.rotate import RandomRotate90\n",
    "from albumentations.augmentations.transforms import HorizontalFlip, VerticalFlip\n",
    "from albumentations.core.composition import OneOf\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "from segmentation_models_pytorch.encoders import get_preprocessing_fn\n",
    "\n",
    "from pytorch_toolbelt import losses\n",
    "\n",
    "from sklearn.model_selection import GroupKFold, KFold\n",
    "\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# 시각화를 위한 라이브러리\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "from matplotlib.patches import Patch\n",
    "import webcolors\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['axes.grid'] = False\n",
    "\n",
    "print('Pytorch version: {}'.format(torch.__version__))\n",
    "print('GPU: {}'.format(torch.cuda.is_available()))\n",
    "\n",
    "print('Device name: ', torch.cuda.get_device_name(0))\n",
    "print('Device count: ', torch.cuda.device_count())\n",
    "\n",
    "# GPU 사용 가능 여부에 따라 device 정보 저장\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Oct 20 17:13:03 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.80.02    Driver Version: 450.80.02    CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  Off  | 00000000:00:05.0 Off |                  Off |\n",
      "| N/A   37C    P0    52W / 300W |  10531MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Set Configs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'batch_size':16,\n",
    "    'num_epochs':25,\n",
    "    'lr':1e-4,\n",
    "    'seed':21,\n",
    "    'n_classes':11,\n",
    "    'experiment_number':'1020',\n",
    "    'encoder_model':'resnext50_32x4d'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Utils**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def _fast_hist(label_true, label_pred, n_class):\n",
    "    mask = (label_true >= 0) & (label_true < n_class)\n",
    "    hist = np.bincount(n_class * label_true[mask].astype(int) + label_pred[mask],\n",
    "                        minlength=n_class ** 2).reshape(n_class, n_class) \n",
    "    return hist\n",
    "\n",
    "\n",
    "def label_accuracy_score(hist):\n",
    "    \"\"\"\n",
    "    Returns accuracy score evaluation result.\n",
    "      - [acc]: overall accuracy\n",
    "      - [acc_cls]: mean accuracy\n",
    "      - [mean_iu]: mean IU\n",
    "      - [fwavacc]: fwavacc\n",
    "    \"\"\"\n",
    "    acc = np.diag(hist).sum() / hist.sum()\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        acc_cls = np.diag(hist) / hist.sum(axis=1)\n",
    "    acc_cls = np.nanmean(acc_cls)\n",
    "\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        iu = np.diag(hist) / (hist.sum(axis=1) + hist.sum(axis=0) - np.diag(hist))\n",
    "    mean_iu = np.nanmean(iu)\n",
    "\n",
    "    freq = hist.sum(axis=1) / hist.sum()\n",
    "    fwavacc = (freq[freq > 0] * iu[freq > 0]).sum()\n",
    "    return acc, acc_cls, mean_iu, fwavacc, iu\n",
    "\n",
    "\n",
    "def add_hist(hist, label_trues, label_preds, n_class):\n",
    "    \"\"\"\n",
    "        stack hist(confusion matrix)\n",
    "    \"\"\"\n",
    "\n",
    "    for lt, lp in zip(label_trues, label_preds):\n",
    "        hist += _fast_hist(lt.flatten(), lp.flatten(), n_class)\n",
    "\n",
    "    return hist\n",
    "\n",
    "\n",
    "def seed_everythind(seed):\n",
    "    \"\"\"\n",
    "        Fix seed\n",
    "    \"\"\"\n",
    "    random_seed = seed\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed) # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(random_seed)\n",
    "    random.seed(random_seed)\n",
    "\n",
    "\n",
    "class_colormap = pd.read_csv(\"../class_dict.csv\")\n",
    "class_colormap\n",
    "\n",
    "def create_trash_label_colormap():\n",
    "    \"\"\"Creates a label colormap used in Trash segmentation.\n",
    "    Returns:\n",
    "        A colormap for visualizing segmentation results.\n",
    "    \"\"\"\n",
    "    colormap = np.zeros((11, 3), dtype=np.uint8)\n",
    "    for inex, (_, r, g, b) in enumerate(class_colormap.values):\n",
    "        colormap[inex] = [r, g, b]\n",
    "    \n",
    "    return colormap\n",
    "\n",
    "\n",
    "def label_to_color_image(label):\n",
    "    \"\"\"Adds color defined by the dataset colormap to the label.\n",
    "\n",
    "    Args:\n",
    "        label: A 2D array with integer type, storing the segmentation label.\n",
    "\n",
    "    Returns:\n",
    "        result: A 2D array with floating type. The element of the array\n",
    "                is the color indexed by the corresponding element in the input label\n",
    "                to the trash color map.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If label is not of rank 2 or its value is larger than color\n",
    "              map maximum entry.\n",
    "    \"\"\"\n",
    "    if label.ndim != 2:\n",
    "        raise ValueError('Expect 2-D input label')\n",
    "\n",
    "    colormap = create_trash_label_colormap()\n",
    "\n",
    "    if np.max(label) >= len(colormap):\n",
    "        raise ValueError('label value too large.')\n",
    "\n",
    "    return colormap[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everythind(CFG['seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Define Dataframe(df)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Categories</th>\n",
       "      <th>Number of annotations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Backgroud</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>General trash</td>\n",
       "      <td>2782.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Paper</td>\n",
       "      <td>9311.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Paper pack</td>\n",
       "      <td>659.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Metal</td>\n",
       "      <td>562.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Glass</td>\n",
       "      <td>610.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Plastic</td>\n",
       "      <td>3090.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Styrofoam</td>\n",
       "      <td>1343.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Plastic bag</td>\n",
       "      <td>7643.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Battery</td>\n",
       "      <td>63.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Clothing</td>\n",
       "      <td>177.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Categories  Number of annotations\n",
       "0       Backgroud                    NaN\n",
       "1   General trash                 2782.0\n",
       "2           Paper                 9311.0\n",
       "3      Paper pack                  659.0\n",
       "4           Metal                  562.0\n",
       "5           Glass                  610.0\n",
       "6         Plastic                 3090.0\n",
       "7       Styrofoam                 1343.0\n",
       "8     Plastic bag                 7643.0\n",
       "9         Battery                   63.0\n",
       "10       Clothing                  177.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path  = '../../input'\n",
    "train_path = dataset_path + '/train_all.json'\n",
    "val_path = dataset_path + '/val.json'\n",
    "test_path = dataset_path + '/test.json'\n",
    "\n",
    "def data_frame(anns_file_path): # for psudo\n",
    "    with open(anns_file_path, 'r') as f:\n",
    "        dataset = json.loads(f.read())\n",
    "    \n",
    "    categories = dataset['categories']\n",
    "    anns = dataset['annotations']\n",
    "    imgs = dataset['images']\n",
    "\n",
    "    nr_cats = len(categories)\n",
    "    nr_annotations = len(anns)\n",
    "    nr_images = len(imgs)\n",
    "\n",
    "    # Load categories and super categories\n",
    "    cat_names = []\n",
    "    super_cat_names = []\n",
    "    super_cat_ids = {}\n",
    "    super_cat_last_name = ''\n",
    "    nr_super_cats = 0\n",
    "    for cat_it in categories:\n",
    "        cat_names.append(cat_it['name'])\n",
    "        super_cat_name = cat_it['supercategory']\n",
    "        # Adding new supercat\n",
    "        if super_cat_name != super_cat_last_name:\n",
    "            super_cat_names.append(super_cat_name)\n",
    "            super_cat_ids[super_cat_name] = nr_super_cats\n",
    "            super_cat_last_name = super_cat_name\n",
    "            nr_super_cats += 1\n",
    "\n",
    "    # Count annotations\n",
    "    cat_histogram = np.zeros(nr_cats,dtype=int)\n",
    "    for ann in anns:\n",
    "        cat_histogram[ann['category_id']-1] += 1\n",
    "\n",
    "    # Initialize the matplotlib figure\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame({'Categories': cat_names, 'Number of annotations': cat_histogram})\n",
    "    df = df.sort_values('Number of annotations', 0, False)\n",
    "    return df\n",
    "\n",
    "\n",
    "# df_t = data_frame(anns_file_path)\n",
    "# df_p = data_frame(psud_file_path)\n",
    "\n",
    "# df = pd.concat([df_t, df_p], ignore_index=True)\n",
    "# df = pd.DataFrame()\n",
    "# for dataframe, colums in df_p:\n",
    "#     if dataframe in df_t['Categories']:\n",
    "#         df.append()\n",
    "# category labeling \n",
    "df = data_frame(train_path)\n",
    "sorted_temp_df = df.sort_index()\n",
    "\n",
    "# background = 0 에 해당되는 label 추가 후 기존들을 모두 label + 1 로 설정\n",
    "sorted_df = pd.DataFrame([\"Backgroud\"], columns = [\"Categories\"])\n",
    "sorted_df = sorted_df.append(sorted_temp_df, ignore_index=True)\n",
    "\n",
    "sorted_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **CustomDataLoader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T06:16:06.631207Z",
     "start_time": "2021-10-04T06:16:06.620206Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "category_names = list(sorted_df.Categories)\n",
    "\n",
    "def get_classname(classID, cats):\n",
    "    for i in range(len(cats)):\n",
    "        if cats[i]['id']==classID:\n",
    "            return cats[i]['name']\n",
    "    return \"None\"\n",
    "\n",
    "class CustomDataLoader(Dataset):\n",
    "    \"\"\"COCO format\"\"\"\n",
    "    def __init__(self, data_dir, mode = 'train', transform = None):\n",
    "        super().__init__()\n",
    "        self.mode = mode\n",
    "        self.transform = transform\n",
    "        self.coco = COCO(data_dir)\n",
    "        \n",
    "    def __getitem__(self, index: int):\n",
    "        # dataset이 index되어 list처럼 동작\n",
    "        image_id = self.coco.getImgIds(imgIds=index)\n",
    "        image_infos = self.coco.loadImgs(image_id)[0]\n",
    "        \n",
    "        # cv2 를 활용하여 image 불러오기\n",
    "        images = cv2.imread(os.path.join(dataset_path, image_infos['file_name']))\n",
    "        images = cv2.cvtColor(images, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        images /= 255.0\n",
    "        if (self.mode in ('train', 'val')):\n",
    "            ann_ids = self.coco.getAnnIds(imgIds=image_infos['id'])\n",
    "            anns = self.coco.loadAnns(ann_ids)\n",
    "\n",
    "            # Load the categories in a variable\n",
    "            cat_ids = self.coco.getCatIds()\n",
    "            cats = self.coco.loadCats(cat_ids)\n",
    "\n",
    "            # masks : size가 (height x width)인 2D\n",
    "            # 각각의 pixel 값에는 \"category id\" 할당\n",
    "            # Background = 0\n",
    "            masks = np.zeros((image_infos[\"height\"], image_infos[\"width\"]))\n",
    "            # General trash = 1, ... , Cigarette = 10\n",
    "            anns = sorted(anns, key=lambda idx : len(idx['segmentation'][0]), reverse=False)\n",
    "            for i in range(len(anns)):\n",
    "                className = get_classname(anns[i]['category_id'], cats)\n",
    "                pixel_value = category_names.index(className)\n",
    "                masks[self.coco.annToMask(anns[i]) == 1] = pixel_value\n",
    "            masks = masks.astype(np.int8)\n",
    "                        \n",
    "            # transform -> albumentations 라이브러리 활용\n",
    "            if self.transform is not None:\n",
    "                transformed = self.transform(image=images, mask=masks)\n",
    "                images = transformed[\"image\"]\n",
    "                masks = transformed[\"mask\"]\n",
    "            return images, masks, image_infos\n",
    "        \n",
    "        if self.mode == 'test':\n",
    "            # transform -> albumentations 라이브러리 활용\n",
    "            if self.transform is not None:\n",
    "                transformed = self.transform(image=images)\n",
    "                images = transformed[\"image\"]\n",
    "            return images, image_infos\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        # 전체 dataset의 size를 return\n",
    "        return len(self.coco.getImgIds())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Dataset and Dataloader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_augmentation(data_dir):\n",
    "    if data_dir == 'train':\n",
    "        return A.Compose([\n",
    "                        OneOf([\n",
    "                            RandomResizedCrop(height=512,width=512,p=1.0),\n",
    "                            HorizontalFlip(p=1.0),\n",
    "                            VerticalFlip(p=1.0),\n",
    "                        ],p=1.0),\n",
    "                        # A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                        A.RandomRotate90(p=1.0),\n",
    "                        A.Cutout(p=0.75),\n",
    "                        ToTensorV2()\n",
    "                        ])\n",
    "    else:\n",
    "        return A.Compose([\n",
    "                        # A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                        ToTensorV2()\n",
    "                        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_dataset = CustomDataLoader(data_dir=train_path, mode='train', transform=get_augmentation('train'))\n",
    "    \n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset, batch_size=CFG['batch_size'], shuffle=True, num_workers=0, collate_fn=collate_fn)\n",
    "        \n",
    "val_dataset = CustomDataLoader(data_dir=val_path, mode='val', transform=get_augmentation('val'))\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset, batch_size=CFG['batch_size'], shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "\n",
    "def inference_dataloader():\n",
    "    test_path = dataset_path + '/test.json'\n",
    "\n",
    "    test_dataset = CustomDataLoader(data_dir=test_path, mode='test', transform=get_augmentation('test'))\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        dataset=test_dataset, batch_size=CFG['batch_size'], num_workers=0, collate_fn=collate_fn)\n",
    "    return test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Define Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T05:11:45.167160Z",
     "start_time": "2021-10-04T05:11:44.782163Z"
    }
   },
   "outputs": [],
   "source": [
    "model = smp.DeepLabV3Plus(\n",
    "    encoder_name=CFG['encoder_model'],\n",
    "    encoder_weights=\"swsl\",\n",
    "    in_channels=3,\n",
    "    classes=11).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T05:11:48.065691Z",
     "start_time": "2021-10-04T05:11:45.412662Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape : torch.Size([2, 3, 512, 512])\n",
      "output shape : torch.Size([2, 11, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn([2, 3, 512, 512]).to(device)\n",
    "print(f\"input shape : {x.shape}\")\n",
    "out = model(x)\n",
    "print(f\"output shape : {out.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **One Epoch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one(epoch, model, train_loader, criterion, optimizer, device):\n",
    "    print(f'Start training..')\n",
    "\n",
    "    model.train()\n",
    "    hist = np.zeros((CFG['n_classes'], CFG['n_classes']))\n",
    "\n",
    "    running_loss = None\n",
    "\n",
    "    pbar = tqdm(enumerate(train_loader), total=len(train_loader), position=0, leave=True)\n",
    "\n",
    "    for step, (images, masks, _) in pbar:\n",
    "\n",
    "        if (step+1) == (len(train_loader)):\n",
    "            continue\n",
    "\n",
    "        images = torch.stack(images).float()       \n",
    "        masks = torch.stack(masks).long() \n",
    "        \n",
    "        # to device\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "        \n",
    "        # Autocast\n",
    "        with autocast():\n",
    "            model = model.to(device)\n",
    "            \n",
    "            # inference\n",
    "            outputs = model(images)\n",
    "        \n",
    "            loss = criterion(outputs, masks)\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            if ((step+1) == (len(train_loader))):\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "        # loss.backward()\n",
    "        # optimizer.step()\n",
    "        \n",
    "        outputs = torch.argmax(outputs, dim=1).detach().cpu().numpy()\n",
    "        masks = masks.detach().cpu().numpy()\n",
    "        \n",
    "        hist = add_hist(hist, masks, outputs, n_class=CFG['n_classes'])\n",
    "        mIoU = label_accuracy_score(hist)[2]\n",
    "        \n",
    "        description = f'epoch {epoch} Loss: {loss.item():.4f} mIoU: {round(mIoU,4)}'\n",
    "        pbar.set_description(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_one(epoch, model, val_loader, criterion, device):\n",
    "    print(f'Start validation #{epoch}')\n",
    "\n",
    "    model.eval()\n",
    "    hist = np.zeros((CFG['n_classes'], CFG['n_classes']))\n",
    "\n",
    "    total_loss = 0\n",
    "    cnt = 0\n",
    "\n",
    "    pbar = tqdm(enumerate(val_loader), total=len(val_loader), position=0, leave=True)\n",
    "\n",
    "    for step, (images, masks, _) in pbar:\n",
    "        images = torch.stack(images)       \n",
    "        masks = torch.stack(masks).long() \n",
    "        \n",
    "        # to device\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "        model = model.to(device)\n",
    "\n",
    "        # inference\n",
    "        outputs = model(images)\n",
    "\n",
    "        loss = criterion(outputs, masks)\n",
    "        total_loss += loss\n",
    "        cnt += 1\n",
    "\n",
    "        outputs = torch.argmax(outputs, dim=1).detach().cpu().numpy()\n",
    "        masks = masks.detach().cpu().numpy()\n",
    "        \n",
    "        hist = add_hist(hist, masks, outputs, n_class=CFG['n_classes'])\n",
    "\n",
    "    acc, acc_cls, mIoU, fwavacc, IoU = label_accuracy_score(hist)\n",
    "    IoU_by_class = [{classes : round(IoU,4)} for IoU, classes in zip(IoU , sorted_df['Categories'])]\n",
    "    \n",
    "    avg_loss = total_loss / cnt\n",
    "\n",
    "    description = f'Validation #{epoch}  Average Loss: {round(avg_loss.item(), 4)}, Accuracy : {round(acc, 4)}, \\\n",
    "                mIoU: {round(mIoU, 4)}'\n",
    "    pbar.set_description(description)                \n",
    "    return avg_loss, mIoU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Make Save dir**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_dir = '../saved/' + CFG['experiment_number']\n",
    "\n",
    "if not os.path.isdir(saved_dir):                                                           \n",
    "    os.mkdir(saved_dir)\n",
    "\n",
    "def save_model(model, saved_dir, file_name=CFG['experiment_number']+CFG['encoder_model']+'.pt'):\n",
    "    check_point = {'net': model.state_dict()}\n",
    "    output_path = os.path.join(saved_dir, file_name)\n",
    "    torch.save(model, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Define Loss and Optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(\n",
    "    params = model.parameters(),\n",
    "    lr = CFG['lr'],\n",
    "    weight_decay = 1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Folds**(not finished)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "FOLDS = 5\n",
    "df[\"Folds\"] = 0\n",
    "\n",
    "kf = KFold(FOLDS, shuffle=True, random_state=CFG['seed'])\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(df)):\n",
    "    df.loc[val_idx, 'Folds'] = fold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 fold start\n",
      "loading annotations into memory...\n",
      "Done (t=3.97s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/164 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done (t=0.87s)\n",
      "creating index...\n",
      "index created!\n",
      "Start training..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 Loss: 2.6329 mIoU: 0.016:  30%|███       | 50/164 [00:53<02:05,  1.10s/it]"
     ]
    }
   ],
   "source": [
    "for fold in range(FOLDS):\n",
    "    print(f'{fold} fold start')\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    train_path = dataset_path + '/train.json'\n",
    "    val_path = dataset_path + '/val.json'\n",
    "\n",
    "    train_dataset = CustomDataLoader(data_dir=train_path, mode='train', transform=get_augmentation('train'))\n",
    "    val_dataset = CustomDataLoader(data_dir=val_path, mode='val', transform=get_augmentation('val'))\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=CFG['batch_size'], shuffle=True, num_workers=0, collate_fn=collate_fn)\n",
    "    val_loader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=CFG['batch_size'], shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "\n",
    "    best_mIoU = 0\n",
    "\n",
    "    for epoch in range(CFG['num_epochs']):\n",
    "        train_one(epoch, model, train_loader, criterion, optimizer, device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            epoch_loss, mIoU = val_one(epoch, model, val_loader, criterion, device)\n",
    "        \n",
    "        if best_mIoU < mIoU:\n",
    "            best_mIoU = mIoU\n",
    "            save_model(model,saved_dir,file_name=CFG['experiment_number']+'_'+CFG['encoder_model']+'.pt')\n",
    "            print('model is saved')\n",
    "            \n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Call .pth**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '../saved/' + CFG['experiment_number'] + '/' + CFG['experiment_number']+'_'+CFG['encoder_model']+'.pt'\n",
    "\n",
    "checkpoint = torch.load(model_path, map_location=device)\n",
    "state_dict = checkpoint.state_dict()\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `plot_examples()` 시각화 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T05:13:53.321160Z",
     "start_time": "2021-10-04T05:13:53.304161Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_examples(mode=\"train\", batch_id=0, num_examples=batch_size, dataloaer=train_loader):\n",
    "    \"\"\"Visualization of images and masks according to batch size\n",
    "    Args:\n",
    "        mode: train/val/test (str)\n",
    "        batch_id : 0 (int) \n",
    "        num_examples : 1 ~ batch_size(e.g. 8) (int)\n",
    "        dataloaer : data_loader (dataloader) \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # variable for legend\n",
    "    category_and_rgb = [[category, (r,g,b)] for idx, (category, r, g, b) in enumerate(class_colormap.values)]\n",
    "    legend_elements = [Patch(facecolor=webcolors.rgb_to_hex(rgb), \n",
    "                             edgecolor=webcolors.rgb_to_hex(rgb), \n",
    "                             label=category) for category, rgb in category_and_rgb]\n",
    "    \n",
    "    # test / validation set에 대한 시각화\n",
    "    if (mode in ('train', 'val')):\n",
    "        with torch.no_grad():\n",
    "            for index, (imgs, masks, image_infos) in enumerate(dataloaer):\n",
    "                if index == batch_id:\n",
    "                    image_infos = image_infos\n",
    "                    temp_images = imgs\n",
    "                    temp_masks = masks\n",
    "\n",
    "                    model.eval()\n",
    "                    # inference\n",
    "                    outs = model(torch.stack(temp_images).to(device))\n",
    "                    oms = torch.argmax(outs, dim=1).detach().cpu().numpy()\n",
    "\n",
    "                    break\n",
    "                else:\n",
    "                    continue\n",
    "    \n",
    "        fig, ax = plt.subplots(nrows=num_examples, ncols=3, figsize=(12, 4*num_examples), constrained_layout=True)\n",
    "        fig.tight_layout()\n",
    "        for row_num in range(num_examples):\n",
    "            # Original Image\n",
    "            ax[row_num][0].imshow(temp_images[row_num].permute([1,2,0]))\n",
    "            ax[row_num][0].set_title(f\"Orignal Image : {image_infos[row_num]['file_name']}\")\n",
    "            # Groud Truth\n",
    "            ax[row_num][1].imshow(label_to_color_image(masks[row_num].detach().cpu().numpy()))\n",
    "            ax[row_num][1].set_title(f\"Groud Truth : {image_infos[row_num]['file_name']}\")\n",
    "            # Pred Mask\n",
    "            ax[row_num][2].imshow(label_to_color_image(oms[row_num]))\n",
    "            ax[row_num][2].set_title(f\"Pred Mask : {image_infos[row_num]['file_name']}\")\n",
    "            ax[row_num][2].legend(handles=legend_elements, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0)\n",
    "        plt.show()\n",
    "    \n",
    "    # test set에 대한 시각화\n",
    "    else :\n",
    "        with torch.no_grad():\n",
    "            for index, (imgs, image_infos) in enumerate(dataloaer):\n",
    "                if index == batch_id:\n",
    "                    image_infos = image_infos\n",
    "                    temp_images = imgs\n",
    "\n",
    "                    model.eval()\n",
    "                    \n",
    "                    # inference\n",
    "                    outs = model(torch.stack(temp_images).to(device))\n",
    "                    oms = torch.argmax(outs, dim=1).detach().cpu().numpy()\n",
    "                    break\n",
    "                else:\n",
    "                    continue\n",
    "    \n",
    "        fig, ax = plt.subplots(nrows=num_examples, ncols=2, figsize=(10, 4*num_examples), constrained_layout=True)\n",
    "\n",
    "        for row_num in range(num_examples):\n",
    "            # Original Image\n",
    "            ax[row_num][0].imshow(temp_images[row_num].permute([1,2,0]))\n",
    "            ax[row_num][0].set_title(f\"Orignal Image : {image_infos[row_num]['file_name']}\")\n",
    "            # Pred Mask\n",
    "            ax[row_num][1].imshow(label_to_color_image(oms[row_num]))\n",
    "            ax[row_num][1].set_title(f\"Pred Mask : {image_infos[row_num]['file_name']}\")\n",
    "            ax[row_num][1].legend(handles=legend_elements, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0)\n",
    "            \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train set 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T05:13:54.065182Z",
     "start_time": "2021-10-04T05:13:54.051662Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_examples(mode=\"train\", batch_id=7, num_examples=4, dataloaer=train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### validation set 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T05:14:19.516160Z",
     "start_time": "2021-10-04T05:14:18.709160Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_examples(mode=\"val\", batch_id=0, num_examples=4, dataloaer=val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test set 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T05:55:27.950201Z",
     "start_time": "2021-10-04T05:55:21.585687Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_examples(mode=\"test\", batch_id=0, num_examples=8, dataloaer=test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## submission을 위한 test 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T06:16:19.666705Z",
     "start_time": "2021-10-04T06:16:19.657706Z"
    }
   },
   "outputs": [],
   "source": [
    "def test(model, data_loader, device):\n",
    "    size = 256\n",
    "    transform = A.Compose([A.Resize(size, size)])\n",
    "    print('Start prediction.')\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    file_name_list = []\n",
    "    preds_array = np.empty((0, size*size), dtype=np.long)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for step, (imgs, image_infos) in enumerate(tqdm(test_loader)):\n",
    "            \n",
    "            # inference (512 x 512)\n",
    "            outs = model(torch.stack(imgs).to(device))\n",
    "            oms = torch.argmax(outs.squeeze(), dim=1).detach().cpu().numpy()\n",
    "            \n",
    "            # resize (256 x 256)\n",
    "            temp_mask = []\n",
    "            for img, mask in zip(np.stack(imgs), oms):\n",
    "                transformed = transform(image=img, mask=mask)\n",
    "                mask = transformed['mask']\n",
    "                temp_mask.append(mask)\n",
    "                \n",
    "            oms = np.array(temp_mask)\n",
    "            \n",
    "            oms = oms.reshape([oms.shape[0], size*size]).astype(int)\n",
    "            preds_array = np.vstack((preds_array, oms))\n",
    "            \n",
    "            file_name_list.append([i['file_name'] for i in image_infos])\n",
    "    print(\"End prediction.\")\n",
    "    file_names = [y for x in file_name_list for y in x]\n",
    "    \n",
    "    return file_names, preds_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## submission.csv 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T06:19:10.926207Z",
     "start_time": "2021-10-04T06:16:20.313208Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sample_submisson.csv 열기\n",
    "submission = pd.read_csv('./submission/sample_submission.csv', index_col=None)\n",
    "\n",
    "# test set에 대한 prediction\n",
    "file_names, preds = test(model, test_loader, device)\n",
    "\n",
    "# PredictionString 대입\n",
    "for file_name, string in zip(file_names, preds):\n",
    "    submission = submission.append({\"image_id\" : file_name, \"PredictionString\" : ' '.join(str(e) for e in string.tolist())}, \n",
    "                                   ignore_index=True)\n",
    "\n",
    "# submission.csv로 저장\n",
    "submission.to_csv(\"./submission/deeplabv3plus_resnext50_epo20.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "interpreter": {
   "hash": "d36e052b391be8c28b05838ade06426769a29575d5fe21a7bc69c7dec0c04c06"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('segmentation': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "394.25px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
